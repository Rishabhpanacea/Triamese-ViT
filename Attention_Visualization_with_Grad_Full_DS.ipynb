{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642ce6a3-a941-43a2-ae94-1a47d171d386",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from Models.MultiViewViT import MultiViewViT\n",
    "from load_data import IMG_Folder\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044bd4ac-b136-49c0-9501-8d8288986cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(w):\n",
    "    classname = w.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        if hasattr(w, 'weight'):\n",
    "            # nn.init.kaiming_normal_(w.weight, mode='fan_out', nonlinearity='relu')\n",
    "            nn.init.kaiming_normal_(w.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "        if hasattr(w, 'bias') and w.bias is not None:\n",
    "                nn.init.constant_(w.bias, 0)\n",
    "    if classname.find('Linear') != -1:\n",
    "        if hasattr(w, 'weight'):\n",
    "            torch.nn.init.xavier_normal_(w.weight)\n",
    "        if hasattr(w, 'bias') and w.bias is not None:\n",
    "            nn.init.constant_(w.bias, 0)\n",
    "    if classname.find('BatchNorm') != -1:\n",
    "        if hasattr(w, 'weight') and w.weight is not None:\n",
    "            nn.init.constant_(w.weight, 1)\n",
    "        if hasattr(w, 'bias') and w.bias is not None:\n",
    "            nn.init.constant_(w.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fd1bc0-cf9a-4d21-a937-0637aa790bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = MultiViewViT(\n",
    "    image_sizes=[(91, 109), (91, 91), (109, 91)],\n",
    "    patch_sizes=[(7, 7), (7, 7), (7, 7)],\n",
    "    num_channals=[91, 109, 91],\n",
    "    vit_args={\n",
    "        'emb_dim': 768, 'mlp_dim': 3072, 'num_heads': 12,\n",
    "        'num_layers': 12, 'num_classes': 1,\n",
    "        'dropout_rate': 0.1, 'attn_dropout_rate': 0.0\n",
    "    },\n",
    "    mlp_dims=[3, 128, 256, 512, 1024, 512, 256, 128, 1]\n",
    ")\n",
    "model.apply(weights_init)\n",
    "model = model.to(\"cpu\")\n",
    "\n",
    "# Load checkpoint\n",
    "CheckpointPath = r'C:\\Users\\Rishabh\\training_output_metricsMulti_VIT_best_model.pth.tar'\n",
    "checkpoint = torch.load(CheckpointPath, map_location=\"cpu\")\n",
    "state_dict = checkpoint[\"state_dict\"]\n",
    "new_state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
    "model.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cff4b8-48f0-4121-bba2-f1a7c8aa9c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "CheckpointPath = r'C:\\Users\\Rishabh\\trainingMulti_VIT_best_model.pth.tar'\n",
    "CSVPath = r'C:\\Users\\Rishabh\\Documents\\TransBTS\\IXI.xlsx'\n",
    "DataFolder = r'C:\\Users\\Rishabh\\Documents\\TrimeseData'\n",
    "device = \"cpu\"\n",
    "Files = os.listdir(DataFolder)\n",
    "ixi_ids = [int(f[3:6]) for f in Files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f90d9d-8ca9-4686-b7b7-a2c45f97211c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(CSVPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d88199-7451-4f87-bb32-a7420295cac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "attentionsv1 = []       # [B,H,T,T] per layer (forward)\n",
    "attention_gradsv1 = []  # [B,H,T,T] per layer (backward)\n",
    "attentionsv2 = []       # [B,H,T,T] per layer (forward)\n",
    "attention_gradsv2 = []  # [B,H,T,T] per layer (backward)\n",
    "attentionsv3 = []       # [B,H,T,T] per layer (forward)\n",
    "attention_gradsv3 = []  # [B,H,T,T] per layer (backward)\n",
    "\n",
    "def hook_attn_probs_v1(module, inp, out):\n",
    "    # out might be probs or (ctx, probs) depending on your code\n",
    "    probs = out[1] if isinstance(out, (tuple, list)) else out\n",
    "    attentionsv1.append(probs)                    # DO NOT detach if you want grads\n",
    "    probs.register_hook(lambda g: attention_gradsv1.append(g))  # tensor-level grad hook\n",
    "\n",
    "def hook_attn_probs_v2(module, inp, out):\n",
    "    # out might be probs or (ctx, probs) depending on your code\n",
    "    probs = out[1] if isinstance(out, (tuple, list)) else out\n",
    "    attentionsv2.append(probs)                    # DO NOT detach if you want grads\n",
    "    probs.register_hook(lambda g: attention_gradsv2.append(g))  # tensor-level grad hook\n",
    "\n",
    "\n",
    "def hook_attn_probs_v3(module, inp, out):\n",
    "    # out might be probs or (ctx, probs) depending on your code\n",
    "    probs = out[1] if isinstance(out, (tuple, list)) else out\n",
    "    attentionsv3.append(probs)                    # DO NOT detach if you want grads\n",
    "    probs.register_hook(lambda g: attention_gradsv3.append(g))  # tensor-level grad hook\n",
    "\n",
    "# Register on each block's attention where probs exist\n",
    "for blk in model.vit_1.transformer.encoder_layers:\n",
    "    blk.attn.register_forward_hook(hook_attn_probs_v1)\n",
    "\n",
    "# Register on each block's attention where probs exist\n",
    "for blk in model.vit_2.transformer.encoder_layers:\n",
    "    blk.attn.register_forward_hook(hook_attn_probs_v2)\n",
    "\n",
    "# Register on each block's attention where probs exist\n",
    "for blk in model.vit_3.transformer.encoder_layers:\n",
    "    blk.attn.register_forward_hook(hook_attn_probs_v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f341ca2-61f4-4018-9f24-fb8a23173a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def white0(image, threshold=0):\n",
    "    \"\"\"\n",
    "    Standardize voxels with value > threshold\n",
    "\n",
    "    Args:\n",
    "        image: Input image\n",
    "        threshold: Threshold value\n",
    "\n",
    "    Returns:\n",
    "        Standardized image\n",
    "    \"\"\"\n",
    "    image = image.astype(np.float32)\n",
    "    mask = (image > threshold).astype(int)\n",
    "\n",
    "    # Vectorized implementation to avoid unnecessary memory allocation\n",
    "    image_h = image * mask\n",
    "\n",
    "    # Calculate mean and std only for relevant voxels\n",
    "    non_zero_voxels = np.sum(mask)\n",
    "    if non_zero_voxels > 0:\n",
    "        mean = np.sum(image_h) / non_zero_voxels\n",
    "\n",
    "        # More memory efficient way to calculate std\n",
    "        std_sum = np.sum((image_h - mean * mask) ** 2)\n",
    "        std = np.sqrt(std_sum / non_zero_voxels)\n",
    "\n",
    "        if std > 0:\n",
    "            normalized = mask * (image - mean) / std\n",
    "            # Use in-place operations to reduce memory usage\n",
    "            image = normalized + image * (1 - mask)\n",
    "            return image\n",
    "\n",
    "    # Default case\n",
    "    return np.zeros_like(image, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec012871-3b12-424d-8ea3-071761b0345a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138dbe44-3e1d-41ea-b3bd-a5af1bcd66e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "model.eval()\n",
    "idx = 16\n",
    "for idx in range(100,  len(Files)):\n",
    "    filename = Files[idx]\n",
    "    file_path = os.path.join(DataFolder, filename)\n",
    "    img = nib.load(file_path)\n",
    "    x_np = img.get_fdata(caching='unchanged').astype(np.float32)       # avoid float64 bloat\n",
    "    # x_tn = torch.from_numpy(x_np).unsqueeze(0).to(device).float()\n",
    "    # print(type(inputvolume), inputvolume.device, inputvolume.shape)\n",
    "    \n",
    "    print(x_np.shape)\n",
    "    \n",
    "    _id = int(filename[3:6])\n",
    "    # AGE = df[df['IXI_ID']==_id]['AGE'].values[0]\n",
    "    \n",
    "    # inputvolume = x_tn.to(device).type(torch.FloatTensor)\n",
    "    inputvolume = white0(x_np)\n",
    "    inputvolume = torch.from_numpy(inputvolume).unsqueeze(0).to(device).float()\n",
    "    inputvolume = inputvolume.to(device).type(torch.FloatTensor)\n",
    "    model.zero_grad(set_to_none=True)\n",
    "\n",
    "    output, (attn1, attn2, attn3) = model(inputvolume, return_attention_weights=True)\n",
    "    output.backward()\n",
    "    Predicted_Age = output.item()\n",
    "    del x_np, inputvolume, output, attn1, attn2, attn3\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997bca12-8c0b-454e-9e19-445ad9adced1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Attnv1 = torch.stack(attentionsv1)\n",
    "AttnGrv1 = torch.stack(attention_gradsv1)\n",
    "Attnv1 = torch.mean(Attnv1, dim=0)\n",
    "AttnGrv1 = torch.mean(AttnGrv1, dim=0)\n",
    "Attnv1 = torch.mean(Attnv1, dim=0)\n",
    "AttnGrv1 = torch.mean(AttnGrv1, dim=0)\n",
    "\n",
    "Attnv2 = torch.stack(attentionsv2)\n",
    "AttnGrv2 = torch.stack(attention_gradsv2)\n",
    "Attnv2 = torch.mean(Attnv2, dim=0)\n",
    "AttnGrv2 = torch.mean(AttnGrv2, dim=0)\n",
    "Attnv2 = torch.mean(Attnv2, dim=0)\n",
    "AttnGrv2 = torch.mean(AttnGrv2, dim=0)\n",
    "\n",
    "\n",
    "Attnv3 = torch.stack(attentionsv3)\n",
    "AttnGrv3 = torch.stack(attention_gradsv3)\n",
    "Attnv3 = torch.mean(Attnv3, dim=0)\n",
    "AttnGrv3 = torch.mean(AttnGrv3, dim=0)\n",
    "Attnv3 = torch.mean(Attnv3, dim=0)\n",
    "AttnGrv3 = torch.mean(AttnGrv3, dim=0)\n",
    "\n",
    "Attnv1.shape, AttnGrv1.shape,Attnv2.shape, AttnGrv2.shape, Attnv3.shape, AttnGrv3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b728df-689a-4742-9e9a-7115780f3c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "def grad_rollout(attentions, gradients, discard_ratio):\n",
    "    result = torch.eye(attentions[0].size(-1))\n",
    "    with torch.no_grad():\n",
    "        for attention, grad in zip(attentions, gradients):                \n",
    "            weights = grad\n",
    "            attention_heads_fused = attention*weights\n",
    "            print(attention_heads_fused.shape)\n",
    "            attention_heads_fused = attention_heads_fused.mean(axis=1)\n",
    "            attention_heads_fused[attention_heads_fused < 0] = 0\n",
    "\n",
    "            # Drop the lowest attentions, but\n",
    "            # don't drop the class token\n",
    "            flat = attention_heads_fused.view(attention_heads_fused.size(0), -1)\n",
    "            _, indices = flat.topk(int(flat.size(-1)*discard_ratio), -1, False)\n",
    "            #indices = indices[indices != 0]\n",
    "            flat[0, indices] = 0\n",
    "\n",
    "            I = torch.eye(attention_heads_fused.size(-1))\n",
    "            a = (attention_heads_fused + 1.0*I)/2\n",
    "            a = a / a.sum(dim=-1)\n",
    "            result = torch.matmul(a, result)\n",
    "    \n",
    "    # Look at the total attention between the class token,\n",
    "    # and the image patches\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb75b8b0-3620-487f-aeda-e85d0dee91fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resultv1 = grad_rollout(Attnv1, AttnGrv1, 0.9)\n",
    "resultv2 = grad_rollout(Attnv2, AttnGrv2, 0.9)\n",
    "resultv3 = grad_rollout(Attnv3, AttnGrv3, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cca457-b8af-4670-b908-9d1ba44587da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "\n",
    "vec1 = resultv1[0, 1:]  # (T_no_cls,)\n",
    "vec2 = resultv2[0, 1:]  # (T_no_cls,)\n",
    "vec3 = resultv3[0, 1:]  # (T_no_cls,)\n",
    "print(len(vec1),len(vec2),len(vec3))\n",
    "mask1 = vec1.reshape(15, 13).detach().numpy()\n",
    "mask1 = cv2.resize(mask1 / mask1.max(), (109, 91))\n",
    "\n",
    "mask2 = vec2.reshape(13, 13).detach().numpy()\n",
    "mask2 = cv2.resize(mask2 / mask2.max(), (91, 91))\n",
    "\n",
    "mask3 = vec3.reshape(13, 15).detach().numpy()\n",
    "mask3 = cv2.resize(mask3 / mask3.max(), (91, 109))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f6bdce-5c03-481d-aa32-8f60125bad5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(mask1, cmap='hot')  # mask is your 2D NumPy array\n",
    "plt.colorbar(label='Attention intensity')\n",
    "plt.title('Attention Map')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd1e81b-92f5-4e6f-99ed-04d8d96ca4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 23\n",
    "filename = Files[idx]\n",
    "file_path = os.path.join(DataFolder, filename)\n",
    "img = nib.load(file_path)\n",
    "x_np = img.get_fdata(caching='unchanged').astype(np.float32)       # avoid float64 bloat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ff6c06-c435-4416-8eef-46c812b113ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Example: pick a middle slice along z-axis\n",
    "slice_idx = x_np.shape[2] // 2  \n",
    "\n",
    "img_slice = x_np[:, :, slice_idx]\n",
    "print(img_slice.shape)\n",
    "mask_slice = mask1\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "# Show base image\n",
    "plt.imshow(img_slice, cmap='gray')\n",
    "\n",
    "# Overlay mask with transparency\n",
    "plt.imshow(mask_slice, cmap='jet', alpha=0.5)  # alpha controls overlay strength\n",
    "plt.colorbar(label=\"Mask intensity\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7fff5c-e045-4a41-a20e-739a1f1539fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Example: pick a middle slice along z-axis\n",
    "slice_idx = x_np.shape[0] // 2  \n",
    "\n",
    "img_slice = x_np[slice_idx, :, :]\n",
    "mask_slice = mask3\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "# Show base image\n",
    "plt.imshow(img_slice, cmap='gray')\n",
    "\n",
    "# Overlay mask with transparency\n",
    "plt.imshow(mask_slice, cmap='jet', alpha=0.5)  # alpha controls overlay strength\n",
    "plt.colorbar(label=\"Mask intensity\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31162eb-9aea-4f94-bcb0-fc00aca7ec28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Example: pick a middle slice along z-axis\n",
    "slice_idx = x_np.shape[0] // 2  \n",
    "\n",
    "img_slice = x_np[:, slice_idx, :]\n",
    "mask_slice = mask2\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "# Show base image\n",
    "plt.imshow(img_slice, cmap='gray')\n",
    "\n",
    "# Overlay mask with transparency\n",
    "plt.imshow(mask_slice, cmap='jet', alpha=0.5)  # alpha controls overlay strength\n",
    "plt.colorbar(label=\"Mask intensity\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6139f95f-c1db-4759-8c6d-469aba5b3967",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b5ba8c-ae5a-49da-a54a-6284df08cd02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
