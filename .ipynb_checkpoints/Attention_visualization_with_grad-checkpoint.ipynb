{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b16279d-4926-45a7-9425-67f48dbc2d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from Models.MultiViewViT import MultiViewViT\n",
    "from load_data import IMG_Folder\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd91d75-0380-41af-b7f3-54d92470c9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(w):\n",
    "    classname = w.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        if hasattr(w, 'weight'):\n",
    "            # nn.init.kaiming_normal_(w.weight, mode='fan_out', nonlinearity='relu')\n",
    "            nn.init.kaiming_normal_(w.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "        if hasattr(w, 'bias') and w.bias is not None:\n",
    "                nn.init.constant_(w.bias, 0)\n",
    "    if classname.find('Linear') != -1:\n",
    "        if hasattr(w, 'weight'):\n",
    "            torch.nn.init.xavier_normal_(w.weight)\n",
    "        if hasattr(w, 'bias') and w.bias is not None:\n",
    "            nn.init.constant_(w.bias, 0)\n",
    "    if classname.find('BatchNorm') != -1:\n",
    "        if hasattr(w, 'weight') and w.weight is not None:\n",
    "            nn.init.constant_(w.weight, 1)\n",
    "        if hasattr(w, 'bias') and w.bias is not None:\n",
    "            nn.init.constant_(w.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6917bdce-f6b3-4a63-ad99-4ac1317dfaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = MultiViewViT(\n",
    "    image_sizes=[(91, 109), (91, 91), (109, 91)],\n",
    "    patch_sizes=[(7, 7), (7, 7), (7, 7)],\n",
    "    num_channals=[91, 109, 91],\n",
    "    vit_args={\n",
    "        'emb_dim': 768, 'mlp_dim': 3072, 'num_heads': 12,\n",
    "        'num_layers': 12, 'num_classes': 1,\n",
    "        'dropout_rate': 0.1, 'attn_dropout_rate': 0.0\n",
    "    },\n",
    "    mlp_dims=[3, 128, 256, 512, 1024, 512, 256, 128, 1]\n",
    ")\n",
    "model.apply(weights_init)\n",
    "model = model.to(\"cpu\")\n",
    "\n",
    "# Load checkpoint\n",
    "CheckpointPath = r'C:\\Users\\Rishabh\\training_output_metricsMulti_VIT_best_model.pth.tar'\n",
    "checkpoint = torch.load(CheckpointPath, map_location=\"cpu\")\n",
    "state_dict = checkpoint[\"state_dict\"]\n",
    "new_state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
    "model.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602f63c2-6d9f-4d04-a640-dc5384033feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "CheckpointPath = r'C:\\Users\\Rishabh\\trainingMulti_VIT_best_model.pth.tar'\n",
    "CSVPath = r'C:\\Users\\Rishabh\\Documents\\TransBTS\\IXI.xlsx'\n",
    "DataFolder = r'C:\\Users\\Rishabh\\Documents\\TrimeseData'\n",
    "device = \"cpu\"\n",
    "Files = os.listdir(DataFolder)\n",
    "ixi_ids = [int(f[3:6]) for f in Files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc84fbd-46ce-467c-9a41-7b6a3b2e2059",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(CSVPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d2d2ed-a1c0-4fb2-a754-23ad18e75025",
   "metadata": {},
   "outputs": [],
   "source": [
    "attentionsv1 = []       # [B,H,T,T] per layer (forward)\n",
    "attention_gradsv1 = []  # [B,H,T,T] per layer (backward)\n",
    "attentionsv2 = []       # [B,H,T,T] per layer (forward)\n",
    "attention_gradsv2 = []  # [B,H,T,T] per layer (backward)\n",
    "attentionsv3 = []       # [B,H,T,T] per layer (forward)\n",
    "attention_gradsv3 = []  # [B,H,T,T] per layer (backward)\n",
    "\n",
    "def hook_attn_probs_v1(module, inp, out):\n",
    "    # out might be probs or (ctx, probs) depending on your code\n",
    "    probs = out[1] if isinstance(out, (tuple, list)) else out\n",
    "    attentionsv1.append(probs)                    # DO NOT detach if you want grads\n",
    "    probs.register_hook(lambda g: attention_gradsv1.append(g))  # tensor-level grad hook\n",
    "\n",
    "def hook_attn_probs_v2(module, inp, out):\n",
    "    # out might be probs or (ctx, probs) depending on your code\n",
    "    probs = out[1] if isinstance(out, (tuple, list)) else out\n",
    "    attentionsv2.append(probs)                    # DO NOT detach if you want grads\n",
    "    probs.register_hook(lambda g: attention_gradsv2.append(g))  # tensor-level grad hook\n",
    "\n",
    "\n",
    "def hook_attn_probs_v3(module, inp, out):\n",
    "    # out might be probs or (ctx, probs) depending on your code\n",
    "    probs = out[1] if isinstance(out, (tuple, list)) else out\n",
    "    attentionsv3.append(probs)                    # DO NOT detach if you want grads\n",
    "    probs.register_hook(lambda g: attention_gradsv3.append(g))  # tensor-level grad hook\n",
    "\n",
    "# Register on each block's attention where probs exist\n",
    "for blk in model.vit_1.transformer.encoder_layers:\n",
    "    blk.attn.register_forward_hook(hook_attn_probs_v1)\n",
    "\n",
    "# Register on each block's attention where probs exist\n",
    "for blk in model.vit_2.transformer.encoder_layers:\n",
    "    blk.attn.register_forward_hook(hook_attn_probs_v2)\n",
    "\n",
    "# Register on each block's attention where probs exist\n",
    "for blk in model.vit_3.transformer.encoder_layers:\n",
    "    blk.attn.register_forward_hook(hook_attn_probs_v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82847d6f-b6b1-4073-96e7-53be07436d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def white0(image, threshold=0):\n",
    "    \"\"\"\n",
    "    Standardize voxels with value > threshold\n",
    "\n",
    "    Args:\n",
    "        image: Input image\n",
    "        threshold: Threshold value\n",
    "\n",
    "    Returns:\n",
    "        Standardized image\n",
    "    \"\"\"\n",
    "    image = image.astype(np.float32)\n",
    "    mask = (image > threshold).astype(int)\n",
    "\n",
    "    # Vectorized implementation to avoid unnecessary memory allocation\n",
    "    image_h = image * mask\n",
    "\n",
    "    # Calculate mean and std only for relevant voxels\n",
    "    non_zero_voxels = np.sum(mask)\n",
    "    if non_zero_voxels > 0:\n",
    "        mean = np.sum(image_h) / non_zero_voxels\n",
    "\n",
    "        # More memory efficient way to calculate std\n",
    "        std_sum = np.sum((image_h - mean * mask) ** 2)\n",
    "        std = np.sqrt(std_sum / non_zero_voxels)\n",
    "\n",
    "        if std > 0:\n",
    "            normalized = mask * (image - mean) / std\n",
    "            # Use in-place operations to reduce memory usage\n",
    "            image = normalized + image * (1 - mask)\n",
    "            return image\n",
    "\n",
    "    # Default case\n",
    "    return np.zeros_like(image, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4e91b6-1450-4614-ac02-9d8d9685fcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "model.eval()\n",
    "idx = 15\n",
    "filename = Files[idx]\n",
    "file_path = os.path.join(DataFolder, filename)\n",
    "img = nib.load(file_path)\n",
    "x_np = img.get_fdata(caching='unchanged').astype(np.float32)       # avoid float64 bloat\n",
    "# x_tn = torch.from_numpy(x_np).unsqueeze(0).to(device).float()\n",
    "# print(type(inputvolume), inputvolume.device, inputvolume.shape)\n",
    "\n",
    "print(x_np.shape)\n",
    "\n",
    "_id = int(filename[3:6])\n",
    "AGE = df[df['IXI_ID']==_id]['AGE'].values[0]\n",
    "\n",
    "# inputvolume = x_tn.to(device).type(torch.FloatTensor)\n",
    "inputvolume = white0(x_np)\n",
    "inputvolume = torch.from_numpy(inputvolume).unsqueeze(0).to(device).float()\n",
    "inputvolume = inputvolume.to(device).type(torch.FloatTensor)\n",
    "output, (attn1, attn2, attn3) = model(inputvolume, return_attention_weights=True)\n",
    "output.backward()\n",
    "Predicted_Age = output.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57ff8a7-4eaa-4e04-9c9e-ed532e81de7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Predicted_Age, AGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f05e4e4-3fcb-44e7-89f8-78ad53e4e68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Attnv1 = torch.stack(attentionsv1)\n",
    "AttnGrv1 = torch.stack(attention_gradsv1)\n",
    "Attnv1 = torch.mean(Attnv1, dim=0)\n",
    "AttnGrv1 = torch.mean(AttnGrv1, dim=0)\n",
    "Attnv1 = torch.mean(Attnv1, dim=0)\n",
    "AttnGrv1 = torch.mean(AttnGrv1, dim=0)\n",
    "\n",
    "Attnv2 = torch.stack(attentionsv2)\n",
    "AttnGrv2 = torch.stack(attention_gradsv2)\n",
    "Attnv2 = torch.mean(Attnv2, dim=0)\n",
    "AttnGrv2 = torch.mean(AttnGrv2, dim=0)\n",
    "Attnv2 = torch.mean(Attnv2, dim=0)\n",
    "AttnGrv2 = torch.mean(AttnGrv2, dim=0)\n",
    "\n",
    "\n",
    "Attnv3 = torch.stack(attentionsv3)\n",
    "AttnGrv3 = torch.stack(attention_gradsv3)\n",
    "Attnv3 = torch.mean(Attnv3, dim=0)\n",
    "AttnGrv3 = torch.mean(AttnGrv3, dim=0)\n",
    "Attnv3 = torch.mean(Attnv3, dim=0)\n",
    "AttnGrv3 = torch.mean(AttnGrv3, dim=0)\n",
    "\n",
    "Attnv1.shape, AttnGrv1.shape,Attnv2.shape, AttnGrv2.shape, Attnv3.shape, AttnGrv3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89d2c7e-658e-4a1a-b107-f5e6c640687b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "def grad_rollout(attentions, gradients, discard_ratio):\n",
    "    result = torch.eye(attentions[0].size(-1))\n",
    "    with torch.no_grad():\n",
    "        for attention, grad in zip(attentions, gradients):                \n",
    "            weights = grad\n",
    "            attention_heads_fused = attention*weights\n",
    "            print(attention_heads_fused.shape)\n",
    "            attention_heads_fused = attention_heads_fused.mean(axis=1)\n",
    "            attention_heads_fused[attention_heads_fused < 0] = 0\n",
    "\n",
    "            # Drop the lowest attentions, but\n",
    "            # don't drop the class token\n",
    "            flat = attention_heads_fused.view(attention_heads_fused.size(0), -1)\n",
    "            _, indices = flat.topk(int(flat.size(-1)*discard_ratio), -1, False)\n",
    "            #indices = indices[indices != 0]\n",
    "            flat[0, indices] = 0\n",
    "\n",
    "            I = torch.eye(attention_heads_fused.size(-1))\n",
    "            a = (attention_heads_fused + 1.0*I)/2\n",
    "            a = a / a.sum(dim=-1)\n",
    "            result = torch.matmul(a, result)\n",
    "    \n",
    "    # Look at the total attention between the class token,\n",
    "    # and the image patches\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd7be1d-16ae-40c4-af0a-d4800598f0ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resultv1 = grad_rollout(Attnv1, AttnGrv1, 0.9)\n",
    "resultv2 = grad_rollout(Attnv2, AttnGrv2, 0.9)\n",
    "resultv3 = grad_rollout(Attnv3, AttnGrv3, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6bc1e9-b2f0-4b87-88c6-ecb1541fc5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "\n",
    "vec1 = resultv1[0, 1:]  # (T_no_cls,)\n",
    "vec2 = resultv2[0, 1:]  # (T_no_cls,)\n",
    "vec3 = resultv3[0, 1:]  # (T_no_cls,)\n",
    "print(len(vec1),len(vec2),len(vec3))\n",
    "mask1 = vec1.reshape(15, 13).detach().numpy()\n",
    "mask1 = cv2.resize(mask1 / mask1.max(), (109, 91))\n",
    "\n",
    "mask2 = vec2.reshape(13, 13).detach().numpy()\n",
    "mask2 = cv2.resize(mask2 / mask2.max(), (91, 91))\n",
    "\n",
    "mask3 = vec3.reshape(13, 15).detach().numpy()\n",
    "mask3 = cv2.resize(mask3 / mask3.max(), (91, 109))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd3c787-602c-436a-bb36-8559b5a68212",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(mask2, cmap='hot')  # mask is your 2D NumPy array\n",
    "plt.colorbar(label='Attention intensity')\n",
    "plt.title('Attention Map')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d05afb0-ff3e-420f-bdf7-4ffe23224075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Example: pick a middle slice along z-axis\n",
    "slice_idx = x_np.shape[2] // 2  \n",
    "\n",
    "img_slice = x_np[:, :, slice_idx]\n",
    "print(img_slice.shape)\n",
    "mask_slice = mask1\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "# Show base image\n",
    "plt.imshow(img_slice, cmap='gray')\n",
    "\n",
    "# Overlay mask with transparency\n",
    "plt.imshow(mask_slice, cmap='jet', alpha=0.5)  # alpha controls overlay strength\n",
    "plt.colorbar(label=\"Mask intensity\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd50fc8e-d12c-4686-939a-77fbbe3c9e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Example: pick a middle slice along z-axis\n",
    "slice_idx = x_np.shape[0] // 2  \n",
    "\n",
    "img_slice = x_np[slice_idx, :, :]\n",
    "mask_slice = mask3\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "# Show base image\n",
    "plt.imshow(img_slice, cmap='gray')\n",
    "\n",
    "# Overlay mask with transparency\n",
    "plt.imshow(mask_slice, cmap='jet', alpha=0.5)  # alpha controls overlay strength\n",
    "plt.colorbar(label=\"Mask intensity\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f27a4c3-2123-4500-a9b1-09e675c83349",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Example: pick a middle slice along z-axis\n",
    "slice_idx = x_np.shape[0] // 2  \n",
    "\n",
    "img_slice = x_np[:, slice_idx, :]\n",
    "mask_slice = mask2\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "# Show base image\n",
    "plt.imshow(img_slice, cmap='gray')\n",
    "\n",
    "# Overlay mask with transparency\n",
    "plt.imshow(mask_slice, cmap='jet', alpha=0.5)  # alpha controls overlay strength\n",
    "plt.colorbar(label=\"Mask intensity\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddd634b-9820-4162-902d-d496b3d11edd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18407d91-05eb-4a9d-af9c-7e545889113e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
