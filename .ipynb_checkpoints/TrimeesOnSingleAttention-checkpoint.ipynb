{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fadda892-c2a1-43f2-9178-89d55d50ccc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from Models.MultiViewViT import MultiViewViT\n",
    "from load_data import IMG_Folder\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28718be5-3a4b-494e-a366-7ec7c4951496",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(w):\n",
    "    classname = w.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        if hasattr(w, 'weight'):\n",
    "            # nn.init.kaiming_normal_(w.weight, mode='fan_out', nonlinearity='relu')\n",
    "            nn.init.kaiming_normal_(w.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "        if hasattr(w, 'bias') and w.bias is not None:\n",
    "                nn.init.constant_(w.bias, 0)\n",
    "    if classname.find('Linear') != -1:\n",
    "        if hasattr(w, 'weight'):\n",
    "            torch.nn.init.xavier_normal_(w.weight)\n",
    "        if hasattr(w, 'bias') and w.bias is not None:\n",
    "            nn.init.constant_(w.bias, 0)\n",
    "    if classname.find('BatchNorm') != -1:\n",
    "        if hasattr(w, 'weight') and w.weight is not None:\n",
    "            nn.init.constant_(w.weight, 1)\n",
    "        if hasattr(w, 'bias') and w.bias is not None:\n",
    "            nn.init.constant_(w.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a706d435-3925-4a64-8322-ac3e2cf9e8ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "model = MultiViewViT(\n",
    "    image_sizes=[(91, 109), (91, 91), (109, 91)],\n",
    "    patch_sizes=[(7, 7), (7, 7), (7, 7)],\n",
    "    num_channals=[91, 109, 91],\n",
    "    vit_args={\n",
    "        'emb_dim': 768, 'mlp_dim': 3072, 'num_heads': 12,\n",
    "        'num_layers': 12, 'num_classes': 1,\n",
    "        'dropout_rate': 0.1, 'attn_dropout_rate': 0.0\n",
    "    },\n",
    "    mlp_dims=[3, 128, 256, 512, 1024, 512, 256, 128, 1]\n",
    ")\n",
    "model.apply(weights_init)\n",
    "model = model.to(\"cpu\")\n",
    "\n",
    "# Load checkpoint\n",
    "CheckpointPath = r'C:\\Users\\Rishabh\\training_output_metricsMulti_VIT_best_model.pth.tar'\n",
    "checkpoint = torch.load(CheckpointPath, map_location=\"cpu\")\n",
    "state_dict = checkpoint[\"state_dict\"]\n",
    "new_state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
    "model.load_state_dict(new_state_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "514d4249-b4cf-48ff-bbed-e3e387fb6779",
   "metadata": {},
   "outputs": [],
   "source": [
    "attentions = []\n",
    "attention_gradients = []\n",
    "def get_attention(module, input, output):\n",
    "    attentions.append(output.cpu())\n",
    "\n",
    "def get_attention_gradient(module, grad_input, grad_output):\n",
    "    print('hii')\n",
    "    # attention_gradients.append(grad_input[0].cpu())\n",
    "    print('grad_input:- ', grad_input)\n",
    "    print('grad_output:- ', grad_output)\n",
    "    attention_gradients.append(grad_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2fa5c27-05f4-4b35-ad6f-ef05bfb20cca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c25c108-7194-4842-bd3b-50f64d6993f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x2023d469290>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vit_1.transformer.encoder_layers[0].attn.register_full_backward_hook(get_attention_gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9a85255-fcc0-4493-8fe8-9f4c004bacf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelfAttention(\n",
       "  (query): LinearGeneral()\n",
       "  (key): LinearGeneral()\n",
       "  (value): LinearGeneral()\n",
       "  (out): LinearGeneral()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vit_1.transformer.encoder_layers[0].attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3db1c4a2-632a-40ce-affc-cb7cae7af1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "CheckpointPath = r'C:\\Users\\Rishabh\\trainingMulti_VIT_best_model.pth.tar'\n",
    "CSVPath = r'C:\\Users\\Rishabh\\Documents\\TransBTS\\IXI.xlsx'\n",
    "DataFolder = r'C:\\Users\\Rishabh\\Documents\\TrimeseData'\n",
    "test_data = IMG_Folder(CSVPath, DataFolder)\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef15f779-735d-47be-951d-087404cd50e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loader = torch.utils.data.DataLoader(test_data\n",
    "                                         ,batch_size=1\n",
    "                                         ,num_workers=0\n",
    "                                         ,pin_memory=True\n",
    "                                         ,drop_last=True\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "663adcc6-2f76-42bc-a4d7-20294bebf724",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m out, targ, ID, Attn1, Attn2, Attn3 = [], [], [], [], [], []\n\u001b[32m      4\u001b[39m target_numpy, predicted_numpy, ID_numpy = [], [], []\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mmodel\u001b[49m.eval()\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m _, (\u001b[38;5;28minput\u001b[39m, ids ,target,male) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(valid_loader):\n\u001b[32m      8\u001b[39m         \u001b[38;5;66;03m# print('ids:- ', ids)\u001b[39;00m\n\u001b[32m      9\u001b[39m         \u001b[38;5;66;03m# print('input:- ', input.shape)\u001b[39;00m\n\u001b[32m     10\u001b[39m         \u001b[38;5;66;03m# print('target:- ', target)\u001b[39;00m\n\u001b[32m     11\u001b[39m         \u001b[38;5;66;03m# print('male:- ', male)\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "out, targ, ID, Attn1, Attn2, Attn3 = [], [], [], [], [], []\n",
    "target_numpy, predicted_numpy, ID_numpy = [], [], []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for _, (input, ids ,target,male) in enumerate(valid_loader):\n",
    "        # print('ids:- ', ids)\n",
    "        # print('input:- ', input.shape)\n",
    "        # print('target:- ', target)\n",
    "        # print('male:- ', male)\n",
    "        \n",
    "        input = input.to(device).type(torch.FloatTensor)\n",
    "        \n",
    "        # ======= convert male lable to one hot type ======= #\n",
    "        # male = torch.unsqueeze(male,1)\n",
    "        # male = torch.zeros(male.shape[0],2).scatter_(1,male,1)\n",
    "        # male = male.type(torch.FloatTensor).to(device)\n",
    "\n",
    "        target = torch.from_numpy(np.expand_dims(target,axis=1))\n",
    "        target = target.type(torch.FloatTensor).to(device)\n",
    "\n",
    "        # ======= compute output and loss ======= #\n",
    "        output, (attn1, attn2, attn3) = model(input, return_attention_weights=True)\n",
    "        out.append(output.cpu().numpy())\n",
    "        targ.append(target.cpu().numpy())\n",
    "        ID.append(ids)\n",
    "        attn1= torch.stack(attn1, dim=0)\n",
    "        attn2 = torch.stack(attn2, dim=0)\n",
    "        attn3 = torch.stack(attn3, dim=0)\n",
    "        print('attn1 shape:- ', attn1.shape)\n",
    "        print('attn2 shape:- ', attn2.shape)\n",
    "        print('attn3 shape:- ', attn3.shape)\n",
    "        attn1_mean = attn1.mean(dim=0)\n",
    "        attn2_mean = attn2.mean(dim=0)\n",
    "        attn3_mean = attn3.mean(dim=0)\n",
    "        avg_attn1 = attn1_mean.mean(dim=1)\n",
    "        avg_attn2 = attn2_mean.mean(dim=1)\n",
    "        avg_attn3 = attn3_mean.mean(dim=1)\n",
    "        avg_attn1 = avg_attn1.mean(dim=0)\n",
    "        avg_attn2 = avg_attn2.mean(dim=0)\n",
    "        avg_attn3 = avg_attn3.mean(dim=0)\n",
    "        Attn1.append(avg_attn1)\n",
    "        Attn2.append(avg_attn2)\n",
    "        Attn3.append(avg_attn3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43f257f2-27c4-4279-b316-59ef9d612253",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Attn1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mAttn1\u001b[49m[\u001b[32m0\u001b[39m].shape, \u001b[38;5;28mlen\u001b[39m(Attn1)\n",
      "\u001b[31mNameError\u001b[39m: name 'Attn1' is not defined"
     ]
    }
   ],
   "source": [
    "Attn1[0].shape, len(Attn1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ce6cfe1-ce83-4e39-9421-bed61ec0785a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m Attn1 = \u001b[43mtorch\u001b[49m.mean(torch.stack(Attn1), dim=\u001b[32m0\u001b[39m)\n\u001b[32m      2\u001b[39m Attn2 = torch.mean(torch.stack(Attn2), dim=\u001b[32m0\u001b[39m)\n\u001b[32m      3\u001b[39m Attn3 = torch.mean(torch.stack(Attn3), dim=\u001b[32m0\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "Attn1 = torch.mean(torch.stack(Attn1), dim=0)\n",
    "Attn2 = torch.mean(torch.stack(Attn2), dim=0)\n",
    "Attn3 = torch.mean(torch.stack(Attn3), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c67d07f-33b3-44b4-a788-98f3acabc437",
   "metadata": {},
   "outputs": [],
   "source": [
    "Attn1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca73f483-a78f-44f7-971f-981705c00002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example: Create a dummy 513x513 attention matrix\n",
    "# In practice, replace this with your actual attention weights\n",
    "attention_matrix = Attn1\n",
    "\n",
    "# Plot the attention matrix\n",
    "plt.figure(figsize=(8, 8))\n",
    "# plt.imshow(attention_matrix, cmap='viridis', interpolation='nearest')\n",
    "plt.imshow(attention_matrix, cmap='hot')\n",
    "\n",
    "plt.colorbar(label=\"Attention Weight\")\n",
    "plt.title(\"Attention Matrix (513 x 513)\")\n",
    "plt.xlabel(\"Key Positions\")\n",
    "plt.ylabel(\"Query Positions\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16539ad4-6908-4c1f-a865-f7d3d7f32804",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example: Create a dummy 513x513 attention matrix\n",
    "# In practice, replace this with your actual attention weights\n",
    "attention_matrix = Attn2\n",
    "\n",
    "# Plot the attention matrix\n",
    "plt.figure(figsize=(8, 8))\n",
    "# plt.imshow(attention_matrix, cmap='viridis', interpolation='nearest')\n",
    "plt.imshow(attention_matrix, cmap='hot')\n",
    "\n",
    "plt.colorbar(label=\"Attention Weight\")\n",
    "plt.title(\"Attention Matrix (513 x 513)\")\n",
    "plt.xlabel(\"Key Positions\")\n",
    "plt.ylabel(\"Query Positions\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64320cec-fc41-4761-9d6d-b6a3736dd06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e27d59-9c3d-4aac-90af-19ea8281fa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example: Create a dummy 513x513 attention matrix\n",
    "# In practice, replace this with your actual attention weights\n",
    "attention_matrix = Attn3\n",
    "\n",
    "# Plot the attention matrix\n",
    "plt.figure(figsize=(16, 16))\n",
    "# plt.imshow(attention_matrix, cmap='viridis', interpolation='nearest')\n",
    "plt.imshow(attention_matrix, cmap='hot')\n",
    "\n",
    "plt.colorbar(label=\"Attention Weight\")\n",
    "plt.title(\"Attention Matrix (513 x 513)\")\n",
    "plt.xlabel(\"Key Positions\")\n",
    "plt.ylabel(\"Query Positions\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998c1401-ca0f-46e6-b4e1-4dc526c6366e",
   "metadata": {},
   "outputs": [],
   "source": [
    "att_mat = Attn3\n",
    "residual_att = torch.eye(att_mat.size(1)).to(device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b6189a-9d37-4e8e-a189-14b83089d01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example: Create a dummy 513x513 attention matrix\n",
    "# In practice, replace this with your actual attention weights\n",
    "attention_matrix = residual_att\n",
    "\n",
    "# Plot the attention matrix\n",
    "plt.figure(figsize=(32, 32))\n",
    "# plt.imshow(attention_matrix, cmap='viridis', interpolation='nearest')\n",
    "plt.imshow(attention_matrix, cmap='hot')\n",
    "\n",
    "plt.colorbar(label=\"Attention Weight\")\n",
    "plt.title(\"Attention Matrix (513 x 513)\")\n",
    "plt.xlabel(\"Key Positions\")\n",
    "plt.ylabel(\"Query Positions\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7571721a-4299-4947-96db-ee02a5c410c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_att_mat = att_mat + residual_att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1bd0ed-bc1f-4d80-a86d-af92d4cd6b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example: Create a dummy 513x513 attention matrix\n",
    "# In practice, replace this with your actual attention weights\n",
    "attention_matrix = aug_att_mat\n",
    "\n",
    "# Plot the attention matrix\n",
    "plt.figure(figsize=(32, 32))\n",
    "# plt.imshow(attention_matrix, cmap='viridis', interpolation='nearest')\n",
    "plt.imshow(attention_matrix, cmap='hot')\n",
    "\n",
    "plt.colorbar(label=\"Attention Weight\")\n",
    "plt.title(\"Attention Matrix (513 x 513)\")\n",
    "plt.xlabel(\"Key Positions\")\n",
    "plt.ylabel(\"Query Positions\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72672040-aac2-424c-b55a-733c17bcfe93",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_att_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1945f437-8add-49a0-a836-857504ac2fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_att_mat = aug_att_mat / aug_att_mat.sum(dim=-1).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b76da2-0c10-4fb3-b18e-bfcf12e7db87",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_att_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b0379e-b365-4ce6-a5cd-2cea912563ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_att_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e188e9-5a2e-4468-9181-43d79d9ed503",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_att_mat.sum(dim=-1).unsqueeze(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551bcf40-1e91-44e5-926e-5407f8309a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example: Create a dummy 513x513 attention matrix\n",
    "# In practice, replace this with your actual attention weights\n",
    "attention_matrix = aug_att_mat\n",
    "\n",
    "# Plot the attention matrix\n",
    "plt.figure(figsize=(32, 32))\n",
    "# plt.imshow(attention_matrix, cmap='viridis', interpolation='nearest')\n",
    "plt.imshow(attention_matrix, cmap='hot')\n",
    "\n",
    "plt.colorbar(label=\"Attention Weight\")\n",
    "plt.title(\"Attention Matrix (513 x 513)\")\n",
    "plt.xlabel(\"Key Positions\")\n",
    "plt.ylabel(\"Query Positions\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5911465a-04d6-44a8-b5a7-adcc8e53b40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_attentions = torch.zeros(aug_att_mat.size()).to(device=\"cpu\")\n",
    "joint_attentions[0] = aug_att_mat[0]\n",
    "for n in range(1, aug_att_mat.size(0)):\n",
    "    joint_attentions[n] = torch.matmul(aug_att_mat[n], joint_attentions[n - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89fc70f-5c78-4466-98e1-7acff4f34518",
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_attentions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad1f794-ab08-4be8-a178-f6695dcfd5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import matplotlib.colors as mcolors\n",
    "\n",
    "# Example: Create a dummy 513x513 attention matrix\n",
    "# In practice, replace this with your actual attention weights\n",
    "attention_matrix = joint_attentions\n",
    "\n",
    "# Plot the attention matrix\n",
    "plt.figure(figsize=(32, 32))\n",
    "plt.imshow(attention_matrix, cmap='viridis', interpolation='nearest')\n",
    "# plt.imshow(attention_matrix, cmap='hot', interpolation='nearest', aspect='equal')\n",
    "# plt.imshow(attention_matrix, cmap='hot', interpolation='nearest',\n",
    "#            norm=mcolors.LogNorm(vmin=attention_matrix.min() + 1e-8, vmax=attention_matrix.max()))\n",
    "plt.colorbar(label=\"Attention Weight\")\n",
    "plt.title(\"Attention Matrix (513 x 513)\")\n",
    "plt.xlabel(\"Key Positions\")\n",
    "plt.ylabel(\"Query Positions\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3d7567-0a55-4a7f-b942-71927ecbf200",
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_attentions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0888961-fba6-4242-80fa-e7b12664e138",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "joint_attentions.sum(dim=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f7d4d7-7285-4ba1-84aa-cf0c2d4fca7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "joint_attentions.sum(dim=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20027c3-6d61-4e71-a757-b6fe4bbf493a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "joint_attentions.sum(dim=[0]) + joint_attentions.sum(dim=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fca61c-41d9-49a7-84e4-5f4da9a30496",
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d333dc-c5a9-4117-a025-5afb45ddad12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
