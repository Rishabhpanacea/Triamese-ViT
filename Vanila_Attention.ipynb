{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068c5d05-0096-4b41-b7bd-5be83cb097e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from Models.MultiViewViT import MultiViewViT\n",
    "from load_data import IMG_Folder\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dbff50-cb77-415e-b339-e7e560e215cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(w):\n",
    "    classname = w.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        if hasattr(w, 'weight'):\n",
    "            # nn.init.kaiming_normal_(w.weight, mode='fan_out', nonlinearity='relu')\n",
    "            nn.init.kaiming_normal_(w.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "        if hasattr(w, 'bias') and w.bias is not None:\n",
    "                nn.init.constant_(w.bias, 0)\n",
    "    if classname.find('Linear') != -1:\n",
    "        if hasattr(w, 'weight'):\n",
    "            torch.nn.init.xavier_normal_(w.weight)\n",
    "        if hasattr(w, 'bias') and w.bias is not None:\n",
    "            nn.init.constant_(w.bias, 0)\n",
    "    if classname.find('BatchNorm') != -1:\n",
    "        if hasattr(w, 'weight') and w.weight is not None:\n",
    "            nn.init.constant_(w.weight, 1)\n",
    "        if hasattr(w, 'bias') and w.bias is not None:\n",
    "            nn.init.constant_(w.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2c2e07-0b0a-4db8-a5a3-96c979e19332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = MultiViewViT(\n",
    "    image_sizes=[(91, 109), (91, 91), (109, 91)],\n",
    "    patch_sizes=[(7, 7), (7, 7), (7, 7)],\n",
    "    num_channals=[91, 109, 91],\n",
    "    vit_args={\n",
    "        'emb_dim': 768, 'mlp_dim': 3072, 'num_heads': 12,\n",
    "        'num_layers': 12, 'num_classes': 1,\n",
    "        'dropout_rate': 0.1, 'attn_dropout_rate': 0.0\n",
    "    },\n",
    "    mlp_dims=[3, 128, 256, 512, 1024, 512, 256, 128, 1]\n",
    ")\n",
    "model.apply(weights_init)\n",
    "model = model.to(\"cpu\")\n",
    "\n",
    "# Load checkpoint\n",
    "CheckpointPath = r'C:\\Users\\Rishabh\\training_output_metricsMulti_VIT_best_model.pth.tar'\n",
    "checkpoint = torch.load(CheckpointPath, map_location=\"cpu\")\n",
    "state_dict = checkpoint[\"state_dict\"]\n",
    "new_state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
    "model.load_state_dict(new_state_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2ac851-a921-4362-8333-b2780451f35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "CheckpointPath = r'C:\\Users\\Rishabh\\trainingMulti_VIT_best_model.pth.tar'\n",
    "CSVPath = r'C:\\Users\\Rishabh\\Documents\\TransBTS\\IXI.xlsx'\n",
    "DataFolder = r'C:\\Users\\Rishabh\\Documents\\TrimeseData'\n",
    "device = \"cpu\"\n",
    "Files = os.listdir(DataFolder)\n",
    "ixi_ids = [int(f[3:6]) for f in Files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42ec944-50f9-4e34-ac1d-e25d38e59fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(CSVPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5447790-ff6f-4449-9bda-386051fd9a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def white0(image, threshold=0):\n",
    "    \"\"\"\n",
    "    Standardize voxels with value > threshold\n",
    "\n",
    "    Args:\n",
    "        image: Input image\n",
    "        threshold: Threshold value\n",
    "\n",
    "    Returns:\n",
    "        Standardized image\n",
    "    \"\"\"\n",
    "    image = image.astype(np.float32)\n",
    "    mask = (image > threshold).astype(int)\n",
    "\n",
    "    # Vectorized implementation to avoid unnecessary memory allocation\n",
    "    image_h = image * mask\n",
    "\n",
    "    # Calculate mean and std only for relevant voxels\n",
    "    non_zero_voxels = np.sum(mask)\n",
    "    if non_zero_voxels > 0:\n",
    "        mean = np.sum(image_h) / non_zero_voxels\n",
    "\n",
    "        # More memory efficient way to calculate std\n",
    "        std_sum = np.sum((image_h - mean * mask) ** 2)\n",
    "        std = np.sqrt(std_sum / non_zero_voxels)\n",
    "\n",
    "        if std > 0:\n",
    "            normalized = mask * (image - mean) / std\n",
    "            # Use in-place operations to reduce memory usage\n",
    "            image = normalized + image * (1 - mask)\n",
    "            return image\n",
    "\n",
    "    # Default case\n",
    "    return np.zeros_like(image, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d33fa37-7338-4ee0-8827-75aa6ddf89a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "model.eval()\n",
    "idx = 15\n",
    "filename = Files[idx]\n",
    "file_path = os.path.join(DataFolder, filename)\n",
    "img = nib.load(file_path)\n",
    "x_np = img.get_fdata(caching='unchanged').astype(np.float32)       # avoid float64 bloat\n",
    "# x_tn = torch.from_numpy(x_np).unsqueeze(0).to(device).float()\n",
    "# print(type(inputvolume), inputvolume.device, inputvolume.shape)\n",
    "\n",
    "print(x_np.shape)\n",
    "\n",
    "_id = int(filename[3:6])\n",
    "AGE = df[df['IXI_ID']==_id]['AGE'].values[0]\n",
    "\n",
    "# inputvolume = x_tn.to(device).type(torch.FloatTensor)\n",
    "inputvolume = white0(x_np)\n",
    "inputvolume = torch.from_numpy(inputvolume).unsqueeze(0).to(device).float()\n",
    "inputvolume = inputvolume.to(device).type(torch.FloatTensor)\n",
    "with torch.no_grad():\n",
    "    output, (attn1, attn2, attn3) = model(inputvolume, return_attention_weights=True)\n",
    "Predicted_Age = output.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a893f9e3-5aff-4ea0-87e5-f4fe7c7c4367",
   "metadata": {},
   "outputs": [],
   "source": [
    "Predicted_Age, AGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c565841c-6269-4802-a0d6-e93a6c1facc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn1 = torch.stack(attn1)\n",
    "attn2 = torch.stack(attn2)\n",
    "attn3 = torch.stack(attn3)\n",
    "attn1 = torch.mean(attn1, dim=0)\n",
    "attn1 = torch.mean(attn1, dim=0)\n",
    "attn1 = torch.mean(attn1, dim=0)\n",
    "\n",
    "attn2 = torch.mean(attn2, dim=0)\n",
    "attn2 = torch.mean(attn2, dim=0)\n",
    "attn2 = torch.mean(attn2, dim=0)\n",
    "\n",
    "attn3 = torch.mean(attn3, dim=0)\n",
    "attn3 = torch.mean(attn3, dim=0)\n",
    "attn3 = torch.mean(attn3, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c74c67c-6bdf-4feb-b741-336e1435c3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn1.shape, attn2.shape, attn3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc5ac17-f969-4094-a818-e9e5b8ac6fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "att_mat = attn1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ba4422-53a4-4f1e-a00e-658d2aa0bb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "att_mat = attn1\n",
    "residual_att = torch.eye(att_mat.size(1)).to(device=\"cpu\")\n",
    "aug_att_mat = att_mat + residual_att\n",
    "aug_att_mat = aug_att_mat / aug_att_mat.sum(dim=-1).unsqueeze(-1)\n",
    "\n",
    "# Recursively multiply the weight matrices\n",
    "joint_attentions = torch.zeros(aug_att_mat.size()).to(device=\"cpu\")\n",
    "joint_attentions[0] = aug_att_mat[0]\n",
    "\n",
    "for n in range(1, aug_att_mat.size(0)):\n",
    "    joint_attentions[n] = torch.matmul(aug_att_mat[n], joint_attentions[n - 1])\n",
    "\n",
    "# Attention from the output token to the input space.\n",
    "v = joint_attentions[0,1:].to(device=\"cpu\")\n",
    "print(len(v))\n",
    "mask1 = v.reshape(15, 13).detach().numpy()\n",
    "mask1 = cv2.resize(mask1 / mask1.max(), (109,91))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73e6eac-8878-4fdf-9b1d-3751a86f411d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "att_mat = attn2\n",
    "residual_att = torch.eye(att_mat.size(1)).to(device=\"cpu\")\n",
    "aug_att_mat = att_mat + residual_att\n",
    "aug_att_mat = aug_att_mat / aug_att_mat.sum(dim=-1).unsqueeze(-1)\n",
    "\n",
    "# Recursively multiply the weight matrices\n",
    "joint_attentions = torch.zeros(aug_att_mat.size()).to(device=\"cpu\")\n",
    "joint_attentions[0] = aug_att_mat[0]\n",
    "\n",
    "for n in range(1, aug_att_mat.size(0)):\n",
    "    joint_attentions[n] = torch.matmul(aug_att_mat[n], joint_attentions[n - 1])\n",
    "\n",
    "# Attention from the output token to the input space.\n",
    "v = joint_attentions[0,1:].to(device=\"cpu\")\n",
    "print(len(v))\n",
    "mask2 = v.reshape(13, 13).detach().numpy()\n",
    "mask2 = cv2.resize(mask2 / mask2.max(), (91,91))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2269d1c4-bde0-42ab-91c4-035941200787",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "att_mat = attn3\n",
    "residual_att = torch.eye(att_mat.size(1)).to(device=\"cpu\")\n",
    "aug_att_mat = att_mat + residual_att\n",
    "aug_att_mat = aug_att_mat / aug_att_mat.sum(dim=-1).unsqueeze(-1)\n",
    "\n",
    "# Recursively multiply the weight matrices\n",
    "joint_attentions = torch.zeros(aug_att_mat.size()).to(device=\"cpu\")\n",
    "joint_attentions[0] = aug_att_mat[0]\n",
    "\n",
    "for n in range(1, aug_att_mat.size(0)):\n",
    "    joint_attentions[n] = torch.matmul(aug_att_mat[n], joint_attentions[n - 1])\n",
    "\n",
    "# Attention from the output token to the input space.\n",
    "v = joint_attentions[0,1:].to(device=\"cpu\")\n",
    "print(len(v))\n",
    "mask3 = v.reshape(13, 15).detach().numpy()\n",
    "mask3 = cv2.resize(mask3 / mask3.max(), (109,91))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7123503-69a7-46c8-b6a0-4a9330852ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask1.shape, mask2.shape, mask3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2712a49b-f0eb-4188-ae62-9a02e868129c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(mask3, cmap='hot')  # mask is your 2D NumPy array\n",
    "plt.colorbar(label='Attention intensity')\n",
    "plt.title('Attention Map')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9665222e-54c5-4edd-a3f2-ca940da1ae9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Example: pick a middle slice along z-axis\n",
    "slice_idx = x_np.shape[2] // 2  \n",
    "\n",
    "img_slice = x_np[:, :, slice_idx]\n",
    "print(img_slice.shape)\n",
    "mask_slice = mask1\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "# Show base image\n",
    "plt.imshow(img_slice, cmap='gray')\n",
    "\n",
    "# Overlay mask with transparency\n",
    "plt.imshow(mask_slice, cmap='jet', alpha=0.5)  # alpha controls overlay strength\n",
    "plt.colorbar(label=\"Mask intensity\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10361ebf-abf5-4080-83b4-a94d0478acca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Example: pick a middle slice along z-axis\n",
    "slice_idx = x_np.shape[0] // 2  \n",
    "\n",
    "img_slice = x_np[slice_idx, :, :]\n",
    "mask_slice = mask3\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "# Show base image\n",
    "plt.imshow(img_slice, cmap='gray')\n",
    "\n",
    "# Overlay mask with transparency\n",
    "plt.imshow(mask_slice, cmap='jet', alpha=0.5)  # alpha controls overlay strength\n",
    "plt.colorbar(label=\"Mask intensity\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd078793-3387-4c5d-9b8f-75dd5d717a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Example: pick a middle slice along z-axis\n",
    "slice_idx = x_np.shape[0] // 2  \n",
    "\n",
    "img_slice = x_np[:, slice_idx, :]\n",
    "mask_slice = mask2\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "# Show base image\n",
    "plt.imshow(img_slice, cmap='gray')\n",
    "\n",
    "# Overlay mask with transparency\n",
    "plt.imshow(mask_slice, cmap='jet', alpha=0.5)  # alpha controls overlay strength\n",
    "plt.colorbar(label=\"Mask intensity\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bf2259-f198-4bda-baa4-7f5dfaee93d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952aec00-7109-401c-9c32-2250d8a7cc03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
